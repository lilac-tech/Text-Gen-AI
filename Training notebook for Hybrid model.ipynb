{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "    - implement a layer that pads char embeddings to match word model's output to reduce gradients calculations and increase training speed\n",
    "      (word embeddings need to be larger than char embeddings cause char embeddings don't need to be large to be represented in the model)\n",
    "            Possible solutions:\n",
    "                1. nn.rnn.pad_sequence can be used\n",
    "                2. custom padding logic can be implemented using nn.functional.pad    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.sparse\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"wikisent2.txt\"\n",
    "BATCH_SIZE = 128\n",
    "MODEL_SAVE_PATH = \"pytorch_model_saves\"\n",
    "VOCABULARY_SAVE_PATH = \"pytorch_vocab_saves\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is cuda available\n",
    "print(\"is cuda available: \", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"cuda cache cleared\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_utils_module import LazyCustomDataset\n",
    "\n",
    "dataset = LazyCustomDataset(DATASET_PATH, random_data=True, max_seq_len=25)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_utils_module import CustomDatasetForTextVectorizer as CD\n",
    "from torch_utils_module import TextVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the TextVectorization class\n",
    "char_encoder = TextVectorizer(max_tokens=None, lower=True, strip_punctuation=False)\n",
    "word_encoder = TextVectorizer(max_tokens=None, split=\" \", lower=True, strip_punctuation=True)\n",
    "\n",
    "#old_wiki2\n",
    "char_vocabulary_save_path = VOCABULARY_SAVE_PATH + \"/reduced_chars_new_full.pth\"\n",
    "word_vocabulary_save_path = VOCABULARY_SAVE_PATH + \"/reduced_words_new_full.pth\"\n",
    "\n",
    "if os.path.exists(char_vocabulary_save_path):\n",
    "    char_encoder.load_vocabulary(char_vocabulary_save_path)\n",
    "    print(\"Char vocabulary loaded successfully\")\n",
    "else:\n",
    "    dataset_for_encoder = CD(\"wikisent2.txt\")\n",
    "    print(\"Adapting vocabulary for chars\")\n",
    "    char_encoder.adapt(dataset_for_encoder, on_labels=True)\n",
    "    char_encoder.prune_vocab(250)\n",
    "    del dataset_for_encoder\n",
    "    char_encoder.save_vocabulary(char_vocabulary_save_path)\n",
    "\n",
    "if os.path.exists(word_vocabulary_save_path):\n",
    "    word_encoder.load_vocabulary(word_vocabulary_save_path)\n",
    "    print(\"Word vocabulary loaded successfully\")\n",
    "else:\n",
    "    dataset_for_encoder = CD(\"wikisent2.txt\")\n",
    "    print(\"Adapting vocabulary for words\")\n",
    "    word_encoder.adapt(dataset_for_encoder, on_labels=True)\n",
    "    word_encoder.prune_vocab(250)\n",
    "    del dataset_for_encoder\n",
    "    word_encoder.save_vocabulary(word_vocabulary_save_path)\n",
    "\n",
    "# Get vocabulary sizes\n",
    "max_vocab_size_char = len(char_encoder.get_vocabulary()[0])\n",
    "max_vocab_size_word = len(word_encoder.get_vocabulary()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if vocabulary sizes are correct\n",
    "max(word_encoder.get_vocabulary()[0].values())+1 == len(word_encoder.get_vocabulary()[0].values()), max(char_encoder.get_vocabulary()[0].values())+1 == len(char_encoder.get_vocabulary()[0].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size_char, max_vocab_size_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_utils_module import Model\n",
    "\n",
    "# Model hyperparameters\n",
    "word_embed_dim = 256\n",
    "char_embed_dim = 128\n",
    "word_rnn_dim, word_rnn_layers = 256, 2\n",
    "char_rnn_dim, char_rnn_layers = 256, 2\n",
    "word_bidirectional, char_bidirectional = False, False\n",
    "word_dense_dims, char_dense_dims = [128, 128], []\n",
    "\n",
    "model_word = Model(\n",
    "    in_embedding_dim=word_embed_dim,\n",
    "    pretrained_embedding_path=None,\n",
    "    out_embedding_dim=char_embed_dim,\n",
    "    rnn_dim=word_rnn_dim,\n",
    "    num_rnn_layers=word_rnn_layers,\n",
    "    rnn_dropout=0,\n",
    "    bidirectional=word_bidirectional,\n",
    "    dense_dims=word_dense_dims,\n",
    "    vocab_size=max_vocab_size_word,\n",
    "    mode=\"word\"\n",
    ")\n",
    "model_char = Model(\n",
    "    in_embedding_dim=char_embed_dim,\n",
    "    out_embedding_dim=None,\n",
    "    rnn_dim=char_rnn_dim,\n",
    "    num_rnn_layers=char_rnn_layers,\n",
    "    rnn_dropout=0,\n",
    "    bidirectional=char_bidirectional,\n",
    "    dense_dims=char_dense_dims,\n",
    "    vocab_size=max_vocab_size_char,\n",
    "    mode=\"char\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the inputs\n",
    "for data in dataloader:\n",
    "    inputs = data[0]\n",
    "    print(\"Processing batch\")\n",
    "    print(\"Batch data:\", data)\n",
    "    print(\"Encoded inputs:\", word_encoder(inputs))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "\n",
    "    print(\"batch\")\n",
    "    print(batch[0])\n",
    "    print(batch[1])\n",
    "\n",
    "    # Encode the word inputs\n",
    "    inputs_W = word_encoder(batch[0])\n",
    "    inputs_W = nn.utils.rnn.pad_sequence(inputs_W, batch_first=True)\n",
    "    inputs_W = inputs_W.to(dtype=torch.int)\n",
    "\n",
    "    # Encode the char inputs\n",
    "    inputs_1 = char_encoder(batch[1])\n",
    "    inputs_1 = nn.utils.rnn.pad_sequence(inputs_1, batch_first=True)\n",
    "    inputs_1 = inputs_1.to(dtype=torch.int)\n",
    "\n",
    "    # Run the models to check for errors\n",
    "    print(\"Word Model:\")\n",
    "    print(\"inputs\", \"inputs_W\", inputs_W.shape)\n",
    "    outputs_= model_word(inputs_W, state=\"training\")\n",
    "    print('outputs_')\n",
    "    print('Training:', 'outputs_', outputs_.shape)\n",
    "    outputs__ = model_word(inputs_W[0], state=\"inference\")\n",
    "    print('Inference:', 'outputs__', outputs__.shape)\n",
    "\n",
    "    print(\"Char Model:\")\n",
    "    print(\"inputs\", \"inputs_1\", inputs_1.shape)\n",
    "    outputs1 = model_char(inputs_1, word_embed_info=outputs_, state=\"training\")\n",
    "    print('Training:', \"outputs1\", outputs1.shape)\n",
    "    outputs11 = model_char(inputs_1[0], word_embed_info=outputs__, state=\"inference\")\n",
    "    print('Inference:', \"outputs11\", outputs11.shape)\n",
    "    \n",
    "    # Run the models to get model inner shapes\n",
    "    model_word(inputs_W, state=\"training\", debug=True)\n",
    "    model_word(inputs_W[0], state=\"training\", debug=True)\n",
    "    model_word(inputs_W, state=\"inference\", debug=True)\n",
    "    model_word(inputs_W[0], state=\"inference\", debug=True)\n",
    "    \n",
    "    model_char(inputs_1, state=\"training\", debug=True)\n",
    "    model_char(inputs_1[0], state=\"training\", debug=True)\n",
    "    model_char(inputs_1, state=\"inference\", debug=True)\n",
    "    model_char(inputs_1[0], state=\"inference\", debug=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify shapes\n",
    "outputs_.shape, outputs__.shape, outputs1.shape, outputs11.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "del inputs_W\n",
    "del inputs_1\n",
    "del outputs_\n",
    "del outputs__\n",
    "del outputs1\n",
    "del outputs11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function\n",
    "def train(model_w, model_c, dataloader, epoch, encoder_W, encoder_C, loss_function, optimizer_W, optimizer_C, device, word_save_path, char_save_path, start_train_at_path, start_train_at):\n",
    "    try:\n",
    "        model_w.to(device)\n",
    "        model_c.to(device)\n",
    "    except:\n",
    "        pass\n",
    "    if start_train_at == 0:\n",
    "        total_loss = 0\n",
    "    else:\n",
    "        total_loss = torch.load(start_train_at_path.replace(\".pt\", \"_loss.pt\"))\n",
    "    loss_plot = []\n",
    "    collected = 0\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=f\"Training Epoch {epoch+1}\", total=len(dataloader), mininterval=5)\n",
    "    i = 1\n",
    "    for inputs_W, targets in pbar:\n",
    "        # skip to 'start_train_at' parameter given by user to resume training\n",
    "        if i <= start_train_at:\n",
    "            i += 1\n",
    "            continue\n",
    "        try:\n",
    "            # Encode the word inputs\n",
    "            inputs_W = encoder_W(inputs_W)\n",
    "            inputs_C = encoder_C(list(map(lambda x: x[:-1], targets)))\n",
    "            targets = encoder_C(list(map(lambda x:  x[1:], targets)))\n",
    "\n",
    "            # Flip the inputs to be right-to-left for word variable so that we can apply pre-padding\n",
    "            inputs_W = list(map(lambda x: x.flip(0), inputs_W))\n",
    "            \n",
    "            # Pad the inputs and flip word variable to be right-to-left : [1, 2, 3, 0, 0] -> [0, 0, 3, 2, 1]\n",
    "            inputs_W = nn.utils.rnn.pad_sequence(inputs_W, batch_first=True, padding_value=encoder_W.vocabulary[\"<pad>\"]).flip(-1).to(device, dtype=torch.int)\n",
    "            inputs_C = nn.utils.rnn.pad_sequence(inputs_C, batch_first=True, padding_value=encoder_C.vocabulary[\"<pad>\"]).to(device, dtype=torch.int)\n",
    "            targets = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=encoder_C.vocabulary[\"<pad>\"]).to(device, dtype=torch.long)\n",
    "            \n",
    "            optimizer_W.zero_grad(set_to_none=True)\n",
    "            optimizer_C.zero_grad(set_to_none=True)\n",
    "\n",
    "            word_info = model_w(inputs_W, state=\"training\")\n",
    "            outputs = model_c(inputs_C, word_embed_info=word_info, state=\"training\")\n",
    "\n",
    "            # Loss calculation\n",
    "            losses = torch.tensor(0, dtype=torch.float, device=device)\n",
    "            output = outputs.view(-1, outputs.shape[-1])\n",
    "            target = targets.view(-1)\n",
    "            losses = torch.masked_select(loss_function(output, target), torch.ne(target, 0)).mean()    # ne means not equal\n",
    "\n",
    "            # Backpropagation\n",
    "            losses.backward()\n",
    "            optimizer_W.step()\n",
    "            optimizer_C.step()\n",
    "\n",
    "            # calculate total_loss for later use\n",
    "            total_loss += losses.detach().cpu().item()\n",
    "        except KeyboardInterrupt:\n",
    "            plt.plot(loss_plot)\n",
    "            plt.show()\n",
    "            raise KeyboardInterrupt\n",
    "        except Exception as e:\n",
    "            plt.plot(loss_plot)\n",
    "            plt.show()\n",
    "            print(\"error in training:\")\n",
    "            a = 10 / 0\n",
    "\n",
    "        try:\n",
    "            if i % 1000 == 0:\n",
    "                # Validation\n",
    "\n",
    "                # Set model to evaluation mode\n",
    "                model_w.eval()\n",
    "                model_c.eval()\n",
    "\n",
    "                seed_text = list(encoder_W.get_vocabulary()[0].keys())[torch.randint(0, len(encoder_W.get_vocabulary()[0]), (1,))]# Get seed text\n",
    "                next_letters = 100\n",
    "                output_letter = \"\"\n",
    "                for _ in range(next_letters):\n",
    "                    seed_text_p1, seed_text_p2 = \" \".join(seed_text.split(\" \")[:-1]), \" \" + seed_text.split(\" \")[-1]\n",
    "\n",
    "                    encoded_seed_text_p1 = encoder_W([seed_text_p1])\n",
    "                    encoded_seed_text_p1 = encoded_seed_text_p1[0].to(device, dtype=torch.int)\n",
    "\n",
    "                    encoded_seed_text_p2 = encoder_C([seed_text_p2])\n",
    "                    encoded_seed_text_p2 = encoded_seed_text_p2[0].to(device, dtype=torch.int)\n",
    "\n",
    "                    output_state = model_w(encoded_seed_text_p1)\n",
    "\n",
    "                    predict_x = model_c(encoded_seed_text_p2, word_embed_info=output_state, state=\"inference\", dropout_allowance=None)\n",
    "\n",
    "                    # this classes_x is based highest probality (model has more confidence)\n",
    "                    #classes_x = torch.argmax(predict_x, dim=-1)\n",
    "\n",
    "                    # this classes_x is based on sampling from the distribution\n",
    "                    classes_x = torch.distributions.Categorical(logits=predict_x).sample()\n",
    "\n",
    "                    output_letter = \"\"\n",
    "                    for index, letter in enumerate(encoder_C.get_vocabulary()[0]):\n",
    "                        if index == classes_x:\n",
    "                            output_letter = letter\n",
    "                            break\n",
    "                    seed_text += output_letter\n",
    "                \n",
    "                # tarcking model learning\n",
    "                with open(\"output.txt\", \"a\") as f:\n",
    "                    f.write(seed_text+\"\\n\")\n",
    "                \n",
    "                # Save models and index into temp path\n",
    "                if i % 1000 == 0:\n",
    "                    path_word = word_save_path\n",
    "                    path_char = char_save_path\n",
    "                    torch.save(model_w.state_dict(), path_word)\n",
    "                    torch.save(model_c.state_dict(), path_char)\n",
    "                    torch.save(i, start_train_at_path)\n",
    "                    torch.save(total_loss, start_train_at_path.replace(\".pt\", \"_loss.pt\"))\n",
    "                    collected = gc.collect()\n",
    "                pbar.set_postfix({'total_loss':float(total_loss/(i)), 'sample':seed_text, 'collected':collected})\n",
    "\n",
    "                # Set model back to training mode\n",
    "                model_w.train()\n",
    "                model_c.train()\n",
    "        except KeyboardInterrupt:\n",
    "            plt.plot(loss_plot)\n",
    "            plt.show()\n",
    "            raise KeyboardInterrupt\n",
    "        except Exception as e:\n",
    "            plt.plot(loss_plot)\n",
    "            plt.show()\n",
    "            print(\"error in sample generation:\")\n",
    "            print(e)\n",
    "        loss_plot.append(total_loss/(i))\n",
    "        i += 1\n",
    "    # Reset saved start_train_at index\n",
    "    torch.save(0, start_train_at_path)\n",
    "    print(f\"Loss: {total_loss/(len(dataloader)/128)}\")\n",
    "    return loss_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "optimizer_word = torch.optim.AdamW(model_word.parameters(), lr=1e-3)\n",
    "optimizer_char = torch.optim.AdamW(model_char.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word.to(device), model_char.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "\n",
    "temp_save_name_word = f\"model_word_{word_embed_dim}_{max_vocab_size_word}_{word_rnn_dim}_{word_rnn_layers}_{word_bidirectional}_{word_dense_dims}_{char_embed_dim}_temp.pth\"\n",
    "temp_save_name_char = f\"model_char_{char_embed_dim}_{max_vocab_size_char}_{char_rnn_dim}_{char_rnn_layers}_{char_bidirectional}_{char_dense_dims}_temp.pth\"\n",
    "train_start_index_file = temp_save_name_char.split(\".\")[0]+temp_save_name_char.split(\".\")[1]+\".pt\"\n",
    "train_start_index = 0\n",
    "\n",
    "if not os.path.exists(os.path.join(MODEL_SAVE_PATH, temp_save_name_word)):\n",
    "    print(\"Model does not exist, creating new model\")\n",
    "    print(\"Starting training from scratch\")\n",
    "    with open(\"output.txt\", \"w\") as f:\n",
    "        f.write(\"\")\n",
    "else:\n",
    "    if input(\"Model already exists, overwrite? (y/n) \") == \"y\":\n",
    "        print(\"Model already exists, overwriting...\")\n",
    "        with open(\"output.txt\", \"w\") as f:\n",
    "            f.write(\"\")\n",
    "        os.remove(os.path.join(MODEL_SAVE_PATH, temp_save_name_word))\n",
    "        os.remove(os.path.join(MODEL_SAVE_PATH, temp_save_name_char))\n",
    "        print(\"Starting training from scratch\")\n",
    "    else:\n",
    "        print(\"Model already exists, loading...\")\n",
    "        model_word.load_state_dict(torch.load(os.path.join(MODEL_SAVE_PATH, temp_save_name_word)))\n",
    "        model_char.load_state_dict(torch.load(os.path.join(MODEL_SAVE_PATH, temp_save_name_char)))\n",
    "        if input(\"Load train start index? (y/n) \") == \"y\":\n",
    "            train_start_index = torch.load(os.path.join(MODEL_SAVE_PATH, train_start_index_file))\n",
    "            print(f\"Starting from index {train_start_index}\")\n",
    "\n",
    "# Training loop   \n",
    "for epoch in range(epochs):\n",
    "    plot_data_for_loss = train(        \n",
    "        model_w=model_word,        \n",
    "        model_c=model_char,        \n",
    "        dataloader=dataloader,        \n",
    "        epoch=epoch,        \n",
    "        encoder_W=word_encoder,        \n",
    "        encoder_C=char_encoder,        \n",
    "        loss_function=loss_function,        \n",
    "        optimizer_W=optimizer_word,        \n",
    "        optimizer_C=optimizer_char,        \n",
    "        device=device,\n",
    "        word_save_path = os.path.join(MODEL_SAVE_PATH, temp_save_name_word),\n",
    "        char_save_path = os.path.join(MODEL_SAVE_PATH, temp_save_name_char),\n",
    "        start_train_at_path = os.path.join(MODEL_SAVE_PATH, train_start_index_file),\n",
    "        start_train_at = train_start_index\n",
    "        )\n",
    "    train_start_index = 0\n",
    "    plt.plot(plot_data_for_loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input(\"Save the model?\"):\n",
    "    save_name_word = f\"model_word_{word_embed_dim}_{max_vocab_size_word}_{word_rnn_dim}_{word_rnn_layers}_{word_bidirectional}_{word_dense_dims}_{char_embed_dim}.pth\"\n",
    "    save_name_char = f\"model_char_{char_embed_dim}_{max_vocab_size_char}_{char_rnn_dim}_{char_rnn_layers}_{char_bidirectional}_{char_dense_dims}.pth\"\n",
    "    print(\"Converting model to cpu...\")\n",
    "    model_char.to(\"cpu\")\n",
    "    model_word.to(\"cpu\")\n",
    "    print(\"Saving model...\")\n",
    "    torch.save(model_word.state_dict(), os.path.join(MODEL_SAVE_PATH, save_name_word))\n",
    "    torch.save(model_char.state_dict(), os.path.join(MODEL_SAVE_PATH, save_name_char))\n",
    "    print(\"Model saved!\")\n",
    "    print(f\"Converting model to {device}...\")\n",
    "    model_char.to(device)\n",
    "    model_word.to(device)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input(\"Load model? (y/n) \") == \"y\":\n",
    "    if input(\"Load latest model in temp save? (y/n) \") == \"y\":\n",
    "        print(\"Loading model from temp model save file\")\n",
    "        model_word.load_state_dict(torch.load(os.path.join(MODEL_SAVE_PATH, temp_save_name_word)))\n",
    "        model_char.load_state_dict(torch.load(os.path.join(MODEL_SAVE_PATH, temp_save_name_char)))\n",
    "        print(\"Model loaded successfully\")\n",
    "        print(f\"Converting model to {device}...\")\n",
    "        model_word.to(device)\n",
    "        model_char.to(device)\n",
    "        print(\"Done\")\n",
    "    else:\n",
    "        print(\"Loading model from model save file\")\n",
    "        model_word.load_state_dict(torch.load(os.path.join(MODEL_SAVE_PATH, save_name_word)))\n",
    "        model_char.load_state_dict(torch.load(os.path.join(MODEL_SAVE_PATH, save_name_char)))\n",
    "        print(\"Model loaded successfully\")\n",
    "        print(f\"Converting model to {device}...\")\n",
    "        model_word.to(device)\n",
    "        model_char.to(device)\n",
    "        print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_text_gen-7dETQLYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
