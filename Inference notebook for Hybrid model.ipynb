{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = \"pytorch_model_saves\"\n",
    "VOCABULARY_SAVE_PATH = \"pytorch_vocab_saves\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is cuda available\n",
    "print(\"is cuda available: \", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"cuda cache cleared\")\n",
    "device = \"cpu\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_utils_module import TextVectorizer\n",
    "\n",
    "# Create instances of the TextVectorization class\n",
    "char_encoder = TextVectorizer(max_tokens=None, lower=True, strip_punctuation=False)\n",
    "word_encoder = TextVectorizer(max_tokens=None, split=\" \", lower=True, strip_punctuation=True)\n",
    "\n",
    "#old_wiki2\n",
    "char_vocabulary_save_path = VOCABULARY_SAVE_PATH + \"/reduced_chars_new_full.pth\"\n",
    "word_vocabulary_save_path = VOCABULARY_SAVE_PATH + \"/reduced_words_new_full.pth\"\n",
    "\n",
    "if os.path.exists(char_vocabulary_save_path):\n",
    "    char_encoder.load_vocabulary(char_vocabulary_save_path)\n",
    "    print(\"Char vocabulary loaded successfully\")\n",
    "else:\n",
    "    raise Exception(\"Char vocabulary not found\")\n",
    "\n",
    "if os.path.exists(word_vocabulary_save_path):\n",
    "    word_encoder.load_vocabulary(word_vocabulary_save_path)\n",
    "    print(\"Word vocabulary loaded successfully\")\n",
    "else:\n",
    "    raise Exception(\"Word vocabulary not found\")\n",
    "\n",
    "# Get vocabulary sizes\n",
    "max_vocab_size_char = len(char_encoder.get_vocabulary()[0])\n",
    "max_vocab_size_word = len(word_encoder.get_vocabulary()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if vocabulary sizes are correct\n",
    "max(word_encoder.get_vocabulary()[0].values())+1 == len(word_encoder.get_vocabulary()[0].values()), max(char_encoder.get_vocabulary()[0].values())+1 == len(char_encoder.get_vocabulary()[0].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size_char, max_vocab_size_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_utils_module import Model\n",
    "\n",
    "# Model hyperparameters\n",
    "word_embed_dim = 256\n",
    "char_embed_dim = 128\n",
    "word_rnn_dim, word_rnn_layers = 256, 2\n",
    "char_rnn_dim, char_rnn_layers = 256, 2\n",
    "word_bidirectional, char_bidirectional = False, False\n",
    "word_dense_dims, char_dense_dims = [128, 128], []\n",
    "\n",
    "model_word = Model(\n",
    "    in_embedding_dim=word_embed_dim,\n",
    "    pretrained_embedding_path=None,\n",
    "    out_embedding_dim=char_embed_dim,\n",
    "    rnn_dim=word_rnn_dim,\n",
    "    num_rnn_layers=word_rnn_layers,\n",
    "    rnn_dropout=0,\n",
    "    bidirectional=word_bidirectional,\n",
    "    dense_dims=word_dense_dims,\n",
    "    vocab_size=max_vocab_size_word,\n",
    "    mode=\"word\"\n",
    ")\n",
    "model_char = Model(\n",
    "    in_embedding_dim=char_embed_dim,\n",
    "    out_embedding_dim=None,\n",
    "    rnn_dim=char_rnn_dim,\n",
    "    num_rnn_layers=char_rnn_layers,\n",
    "    rnn_dropout=0,\n",
    "    bidirectional=char_bidirectional,\n",
    "    dense_dims=char_dense_dims,\n",
    "    vocab_size=max_vocab_size_char,\n",
    "    mode=\"char\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word.to(device), model_char.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "temp_save_path_word = f\"model_word_{word_embed_dim}_{max_vocab_size_word}_{word_rnn_dim}_{word_rnn_layers}_{word_bidirectional}_{word_dense_dims}_{char_embed_dim}_temp.pth\"\n",
    "temp_save_path_char = f\"model_char_{char_embed_dim}_{max_vocab_size_char}_{char_rnn_dim}_{char_rnn_layers}_{char_bidirectional}_{char_dense_dims}_temp.pth\"\n",
    "save_path_word = f\"model_word_{word_embed_dim}_{max_vocab_size_word}_{word_rnn_dim}_{word_rnn_layers}_{word_bidirectional}_{word_dense_dims}_{char_embed_dim}.pth\"\n",
    "save_path_char = f\"model_char_{char_embed_dim}_{max_vocab_size_char}_{char_rnn_dim}_{char_rnn_layers}_{char_bidirectional}_{char_dense_dims}.pth\"\n",
    "\n",
    "if input(\"Load model? (y/n) \") == \"y\":\n",
    "    if input(\"Load latest model in temp save? (y/n) \") == \"y\":\n",
    "        print(\"Loading model from temp model save file\")\n",
    "        model_word.load_state_dict(torch.load(os.path.join(MODEL_SAVE_PATH, temp_save_path_word)))\n",
    "        model_char.load_state_dict(torch.load(os.path.join(MODEL_SAVE_PATH, temp_save_path_char)))\n",
    "        print(\"Model loaded successfully\")\n",
    "        print(f\"Converting model to {device}...\")\n",
    "        model_word.to(device)\n",
    "        model_char.to(device)\n",
    "        print(\"Done\")\n",
    "    else:\n",
    "        print(\"Loading model from model save file\")\n",
    "        model_word.load_state_dict(torch.load(os.path.join(MODEL_SAVE_PATH, save_path_word)))\n",
    "        model_char.load_state_dict(torch.load(os.path.join(MODEL_SAVE_PATH, save_path_char)))\n",
    "        print(\"Model loaded successfully\")\n",
    "        print(f\"Converting model to {device}...\")\n",
    "        model_word.to(device)\n",
    "        model_char.to(device)\n",
    "        print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control parameters\n",
    "device = device            #this line is here so that i can change device from the same cell\n",
    "prompt = \"Politics is \"\n",
    "lines = 10\n",
    "next_words = 10\n",
    "use_distribution = False\n",
    "\n",
    "prompt = prompt.lower()\n",
    "\n",
    "# move model to device and set to eval mode\n",
    "model_word.to(device)\n",
    "model_char.to(device)\n",
    "model_word.eval()\n",
    "model_char.eval()\n",
    "\n",
    "# print info on whether each word in the promt is in the vocabulary\n",
    "print(\"Word\\t\\t\\tIs_in_vocabulary\\t\\tIndex\")\n",
    "for word in prompt.split(\" \"):\n",
    "    if word in word_encoder.get_vocabulary()[0]:\n",
    "        print(f\"{word}\\t\\t\\tO\\t\\t\\t{word_encoder.get_vocabulary()[0][word]}\")\n",
    "    else:\n",
    "        print(f\"{word}\\t\\t\\tX\")\n",
    "\n",
    "# generate text\n",
    "for _ in range(lines):\n",
    "    seed_text = prompt                                                              # set seed text\n",
    "    print(seed_text, end=\"\")\n",
    "    for _ in range(next_words):\n",
    "        # use word model to get sentence meaning\n",
    "        seed_text_p1 = \" \".join(seed_text.split(\" \")[:-1])                          # remove last word (last word will be used in/by char model)\n",
    "        encoded_seed_text_p1 = word_encoder([seed_text_p1])                         # encode seed text\n",
    "        encoded_seed_text_p1 = encoded_seed_text_p1[0].to(device, dtype=torch.int)  # move encoded seed text to device\n",
    "        output_state = model_word(encoded_seed_text_p1, state=\"inference\")          # get sentence meaining from model\n",
    "        output_letter = \"\"                                                          # initialize output letter (will be updated in loop)\n",
    "        while output_letter not in (\"<pad>\", \"<unk>\", \" \"):                         # loop until output letter is not in vocabulary\n",
    "            # use char model to get next letter of the word in generation\n",
    "            seed_text_p2 = seed_text.split(\" \")[-1]                                                                                 # get last word\n",
    "            encoded_seed_text_p2 = char_encoder([seed_text_p2])                                                                     # encode last word\n",
    "            encoded_seed_text_p2 = encoded_seed_text_p2[0].to(device, dtype=torch.int)                                              # move encoded seed text to device\n",
    "            predict_x = model_char(encoded_seed_text_p2, word_embed_info=output_state, dropout_allowance=0.075, state=\"inference\")   # get next letter\n",
    "            if use_distribution:\n",
    "                classes_x = torch.distributions.Categorical(logits=predict_x).sample()                                              # sample next letter based on confidence i.e. highest probability\n",
    "            else:\n",
    "                classes_x = torch.argmax(predict_x).item()                                                                  # sample next letter based on probability distribution\n",
    "            output_letter = \"\"                                                                                                      # Reset output letter\n",
    "            for index, letter in enumerate(char_encoder.get_vocabulary()[0]):                                                       # get letter from vocabulary\n",
    "                if index == classes_x:\n",
    "                    output_letter = letter\n",
    "                    break\n",
    "            \n",
    "            print(output_letter, end=\"\")                                            # print next letter\n",
    "            seed_text += output_letter                                              # update seed text\n",
    "    print()                                                                         # print new line"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_text_gen-7dETQLYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
